{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOMKUODGmDAYKEPDgVGc0ex"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"naqgmGruWck9"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","# Load and preprocess the data\n","df = pd.read_csv('postgres_data_15000_rv_mean.csv')\n","\n","# Define the features and target\n","features = ['rv', 'ae']\n","X = df[features].astype(float).values\n","y = df['ae_cl']\n","\n","# Create a mask that is True for non-nan values and False for nan values\n","mask = ~np.isnan(y)\n","\n","# Apply the mask to both X and y\n","X = X[mask]\n","y = y[mask]\n","\n","# Check the count of each class in y\n","print(np.unique(y, return_counts=True))\n","\n","X = X.reshape(-1, 2)\n","y = y.values.reshape(-1)\n","\n","# Convert the data to tensors\n","X_tensor = torch.Tensor(X)\n","y_tensor = torch.LongTensor(y)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n","\n","# Define the neural network model\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.fc1 = nn.Linear(2, 10)  # Input layer with 1 input feature and 10 hidden units\n","        self.fc2 = nn.Linear(10, 3)  # Hidden layer with 10 units and 3 output units\n","        self.softmax = nn.Softmax(dim=1)  # Softmax layer to get probabilities of each class\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))  # Apply ReLU activation to the hidden layer\n","        x = self.fc2(x)  # Apply linear transformation to the output layer\n","        x = self.softmax(x)  # Apply softmax to get probabilities of each class\n","        return x\n","\n","# Create an instance of the neural network model\n","model = NeuralNetwork()\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001)\n","\n","# Train the model\n","num_epochs = 300\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    outputs = model(X_train)\n","    loss = criterion(outputs, y_train)\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n","\n","\n","# Evaluate the model\n","with torch.no_grad():\n","    train_outputs = model(X_train)\n","    train_predictions = torch.argmax(train_outputs, dim=1)\n","    train_accuracy = (train_predictions == y_train).float().mean()\n","\n","    test_outputs = model(X_test)\n","    test_predictions = torch.argmax(test_outputs, dim=1)\n","    test_accuracy = (test_predictions == y_test).float().mean()\n","\n","\n","print(\"Training Accuracy: {:.2f}%\".format(train_accuracy * 100))\n","print(\"Testing Accuracy: {:.2f}%\".format(test_accuracy * 100))\n","\n","\n","# Create and print new df with actual and predicted value for test set\n","df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': test_predictions.flatten()})\n","print(df)\n","\n","# Save the dataframe to a CSV file\n","df.to_csv('predictions_cl.csv', index=False)\n","\n","\n","# Calculate difference between actual and predicted values\n","df['Difference'] = (df['Actual'] - df['Predicted'])\n","\n","# Calculate accuracy\n","count = (df['Difference'] == 0.00).sum()\n","print(\"Correct predictions:\", count)\n","print(\"Total predictions:\", len(df))\n","print(\"Accuracy:\", count / len(df))\n","\n","# Check the count of each class in df['Predicted']\n","print(np.unique(df['Predicted'], return_counts=True))\n","\n","# NB the model ALWAYS predicts 2, given the high frequency of its occurence in the training set\n","\n"]}]}