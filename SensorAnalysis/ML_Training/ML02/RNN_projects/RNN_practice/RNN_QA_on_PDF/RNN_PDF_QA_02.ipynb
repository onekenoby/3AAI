{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_question_and_answer(model, tokenizer, seed_text, max_sequence_len, num_words_to_generate):\n",
    "    for _ in range(num_words_to_generate):\n",
    "        # Tokenize the seed text\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # Truncate sequences to a fixed length\n",
    "        encoded = pad_sequences(\n",
    "            [encoded], maxlen=max_sequence_len, truncating='pre')\n",
    "        # Predict the next word\n",
    "        y_pred = model.predict_classes(encoded, verbose=0)\n",
    "        # Map predicted word index to word\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_pred:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        # Add predicted word to the seed text for the next prediction\n",
    "        seed_text += \" \" + predicted_word\n",
    "    return seed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to the three PDF files\n",
    "pdf1_path = \"file1.pdf\"\n",
    "#pdf2_path = \"file2.pdf\"\n",
    "pdf3_path = \"file3.pdf\"\n",
    "\n",
    "# Extract text from PDF files\n",
    "pdf1_text = extract_text_from_pdf(pdf1_path)\n",
    "#pdf2_text = extract_text_from_pdf(pdf2_path)\n",
    "pdf3_text = extract_text_from_pdf(pdf3_path)\n",
    "\n",
    "# Combine texts\n",
    "#combined_text = pdf1_text + pdf2_text + pdf3_text\n",
    "combined_text = pdf1_text + pdf3_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([combined_text])\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences([combined_text])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate input sequences and labels\n",
    "input_sequences = []\n",
    "max_sequence_len = 200\n",
    "for i in range(0, len(sequences) - max_sequence_len, 1):\n",
    "    input_seq = sequences[i:i + max_sequence_len]\n",
    "    input_sequences.append(input_seq)\n",
    "\n",
    "# Convert input sequences to numpy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "\n",
    "# Split input sequences into X and y\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# Build the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_sequence_len - 1),\n",
    "    Bidirectional(LSTM(150, return_sequences=True)),\n",
    "    Bidirectional(LSTM(150)),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=128, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "save_model(model, 'pdf_rnn_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the trained model\n",
    "model = load_model('pdf_rnn_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_and_answer(model, tokenizer, seed_text, max_sequence_len, num_words_to_generate):\n",
    "    for _ in range(num_words_to_generate):\n",
    "        # Tokenize the seed text\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # Truncate sequences to a fixed length\n",
    "        encoded = pad_sequences(\n",
    "            [encoded], maxlen=max_sequence_len - 1)#, truncating='pre')\n",
    "        # Predict the next word probabilities\n",
    "        y_pred_probs = model.predict(encoded, verbose=0)[0]\n",
    "        # Get the index of the word with the highest probability\n",
    "        predicted_word_index = np.argmax(y_pred_probs)\n",
    "        # Map predicted word index to word\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        # Add predicted word to the seed text for the next prediction\n",
    "        seed_text += \" \" + predicted_word\n",
    "    # Convert generated text to ASCII\n",
    "    return seed_text.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a question prompt\n",
    "seed_text = input(\"Enter a seed text to generate a question: \")\n",
    "num_words_to_generate = int(\n",
    "    input(\"Enter the number of words to generate in the question: \"))\n",
    "\n",
    "# Generate question and answer using the trained model\n",
    "generated_question = generate_question_and_answer(\n",
    "    model, tokenizer, seed_text, max_sequence_len, num_words_to_generate)\n",
    "\n",
    "# Display the generated question\n",
    "print(\"Generated Question:\", generated_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'pdf_rnn_model.h5')\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('pdf_rnn_model.h5')\n",
    "\n",
    "# Generate a question prompt\n",
    "seed_text = input(\"Enter a seed text to generate a question: \")\n",
    "num_words_to_generate = int(\n",
    "    input(\"Enter the number of words to generate in the question: \"))\n",
    "\n",
    "# Generate question and answer using the trained model\n",
    "generated_question = generate_question_and_answer(\n",
    "    model, tokenizer, seed_text, max_sequence_len, num_words_to_generate)\n",
    "\n",
    "# Display the generated question\n",
    "print(\"Generated Question:\", generated_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3aai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
