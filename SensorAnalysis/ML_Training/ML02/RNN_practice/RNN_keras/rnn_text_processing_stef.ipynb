{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Informative System using a RNN Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import warnings\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Sample documents for initial training (starting with two documents)\n","documents_initial = [\n","    \"This is the first document.\",\n","    \"Here is the second document.\"\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Tokenize the initial documents\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(documents_initial)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Convert text to sequences\n","sequences_initial = tokenizer.texts_to_sequences(documents_initial)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequences_initial"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Pad sequences\n","max_sequence_length = max(len(seq) for seq in sequences_initial)\n","padded_sequences_initial = pad_sequences(\n","    sequences_initial, maxlen=max_sequence_length, padding='post', truncating='post')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Create input data (X) and target data (y) for initial training\n","X_train_initial = padded_sequences_initial[:, :-1]\n","# Target is not one-hot encoded in this approach\n","y_train_initial = padded_sequences_initial[:, 1:]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_initial"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_train_initial"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Define the initial RNN model\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_dim = 50\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","model = Sequential()\n","model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n","          input_length=max_sequence_length - 1))\n","# Return sequences for training purposes\n","model.add(LSTM(units=100, return_sequences=True))\n","model.add(Dense(units=vocab_size, activation='softmax'))\n","\n","# Compile the initial model\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam', metrics=['accuracy'])\n","\n","# Define a ModelCheckpoint callback to save the model during training\n","checkpoint_callback = ModelCheckpoint(\n","    \"initial_model_checkpoint.h5\", save_best_only=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Train the initial model and include the checkpoint callback\n","model.fit(X_train_initial, y_train_initial, epochs=50,\n","          batch_size=1, callbacks=[checkpoint_callback])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Save the entire initial model (including architecture, optimizer, and weights)\n","#model.save(\"initial_model.h5\")\n","\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    model.save('initial_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import numpy as np\n"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["\n","# Sample documents\n","documents = [\n","    \"Document 1 is about machine learning.\",\n","    \"Document 2 talks about natural language processing.\",\n","    \"Document 3 covers recurrent neural networks.\"\n","]\n"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["\n","# Tokenizing the documents\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(documents)\n","total_words = len(tokenizer.word_index) + 1\n"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["\n","# Creating input sequences and labels\n","input_sequences = []\n","for line in documents:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    input_sequences.append(token_list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","max_sequence_length = max(len(seq) for seq in input_sequences)\n","input_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n","    input_sequences, maxlen=max_sequence_length, padding='pre')\n","x, y = input_sequences[:, :-1], input_sequences[:, -1]\n","y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ 0,  1,  3,  4,  2,  5],\n","       [ 1,  7,  8,  2,  9, 10],\n","       [ 0,  1, 12, 13, 14, 15]])"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n","        0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        1.]], dtype=float32)"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n","model.add(LSTM(100))\n","model.add(Dense(total_words, activation='softmax'))\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam', metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","1/1 - 3s - loss: 2.8378 - accuracy: 0.0000e+00 - 3s/epoch - 3s/step\n","Epoch 2/50\n","1/1 - 0s - loss: 2.8241 - accuracy: 0.6667 - 0s/epoch - 0s/step\n","Epoch 3/50\n","1/1 - 0s - loss: 2.8105 - accuracy: 1.0000 - 23ms/epoch - 23ms/step\n","Epoch 4/50\n","1/1 - 0s - loss: 2.7966 - accuracy: 1.0000 - 0s/epoch - 0s/step\n","Epoch 5/50\n","1/1 - 0s - loss: 2.7821 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n","Epoch 6/50\n","1/1 - 0s - loss: 2.7669 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n","Epoch 7/50\n","1/1 - 0s - loss: 2.7506 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n","Epoch 8/50\n","1/1 - 0s - loss: 2.7329 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n","Epoch 9/50\n","1/1 - 0s - loss: 2.7136 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n","Epoch 10/50\n","1/1 - 0s - loss: 2.6924 - accuracy: 1.0000 - 4ms/epoch - 4ms/step\n","Epoch 11/50\n","1/1 - 0s - loss: 2.6689 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n","Epoch 12/50\n","1/1 - 0s - loss: 2.6426 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n","Epoch 13/50\n","1/1 - 0s - loss: 2.6132 - accuracy: 1.0000 - 0s/epoch - 0s/step\n","Epoch 14/50\n","1/1 - 0s - loss: 2.5800 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n","Epoch 15/50\n","1/1 - 0s - loss: 2.5423 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n","Epoch 16/50\n","1/1 - 0s - loss: 2.4994 - accuracy: 1.0000 - 0s/epoch - 0s/step\n","Epoch 17/50\n","1/1 - 0s - loss: 2.4504 - accuracy: 1.0000 - 496us/epoch - 496us/step\n","Epoch 18/50\n","1/1 - 0s - loss: 2.3940 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n","Epoch 19/50\n","1/1 - 0s - loss: 2.3292 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n","Epoch 20/50\n","1/1 - 0s - loss: 2.2542 - accuracy: 1.0000 - 0s/epoch - 0s/step\n","Epoch 21/50\n","1/1 - 0s - loss: 2.1677 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n","Epoch 22/50\n","1/1 - 0s - loss: 2.0677 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n","Epoch 23/50\n","1/1 - 0s - loss: 1.9530 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n","Epoch 24/50\n","1/1 - 0s - loss: 1.8224 - accuracy: 1.0000 - 0s/epoch - 0s/step\n","Epoch 25/50\n","1/1 - 0s - loss: 1.6766 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n","Epoch 26/50\n","1/1 - 0s - loss: 1.5187 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n","Epoch 27/50\n","1/1 - 0s - loss: 1.3562 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n","Epoch 28/50\n","1/1 - 0s - loss: 1.2016 - accuracy: 0.6667 - 0s/epoch - 0s/step\n","Epoch 29/50\n","1/1 - 0s - loss: 1.0702 - accuracy: 0.6667 - 2ms/epoch - 2ms/step\n","Epoch 30/50\n","1/1 - 0s - loss: 0.9730 - accuracy: 0.6667 - 19ms/epoch - 19ms/step\n","Epoch 31/50\n","1/1 - 0s - loss: 0.9095 - accuracy: 0.6667 - 15ms/epoch - 15ms/step\n","Epoch 32/50\n","1/1 - 0s - loss: 0.8681 - accuracy: 0.6667 - 0s/epoch - 0s/step\n","Epoch 33/50\n","1/1 - 0s - loss: 0.8364 - accuracy: 0.6667 - 4ms/epoch - 4ms/step\n","Epoch 34/50\n","1/1 - 0s - loss: 0.8072 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n","Epoch 35/50\n","1/1 - 0s - loss: 0.7794 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n","Epoch 36/50\n","1/1 - 0s - loss: 0.7543 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n","Epoch 37/50\n","1/1 - 0s - loss: 0.7319 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n","Epoch 38/50\n","1/1 - 0s - loss: 0.7087 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n","Epoch 39/50\n","1/1 - 0s - loss: 0.6797 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n","Epoch 40/50\n","1/1 - 0s - loss: 0.6421 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n","Epoch 41/50\n","1/1 - 0s - loss: 0.5973 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n","Epoch 42/50\n","1/1 - 0s - loss: 0.5490 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n","Epoch 43/50\n","1/1 - 0s - loss: 0.5004 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n","Epoch 44/50\n","1/1 - 0s - loss: 0.4530 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n","Epoch 45/50\n","1/1 - 0s - loss: 0.4066 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n","Epoch 46/50\n","1/1 - 0s - loss: 0.3616 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n","Epoch 47/50\n","1/1 - 0s - loss: 0.3188 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n","Epoch 48/50\n","1/1 - 0s - loss: 0.2795 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n","Epoch 49/50\n","1/1 - 0s - loss: 0.2443 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n","Epoch 50/50\n","1/1 - 0s - loss: 0.2131 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x180195ff210>"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Train the model\n","model.fit(x, y, epochs=50, verbose=2)\n"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["\n","# Save the model\n","model.save('my_rnn_model.keras')\n"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["\n","# Load the saved model\n","loaded_model = load_model('my_rnn_model.keras')\n"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["\n","def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n","    for _ in range(next_words):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = tf.keras.preprocessing.sequence.pad_sequences(\n","            [token_list], maxlen=max_sequence_len-1, padding='pre')\n","        predicted_probs = model.predict(token_list, verbose=0)\n","        predicted_index = np.argmax(predicted_probs)\n","        output_word = \"\"\n","        for word, index in tokenizer.word_index.items():\n","            if index == predicted_index:\n","                output_word = word\n","                break\n","        seed_text += \" \" + output_word\n","    return seed_text"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["\n","def reinforce_model(feedback_text, target_word, model, tokenizer, max_sequence_len, train_model=True):\n","    x = tokenizer.texts_to_sequences([feedback_text])[0]\n","    x = tf.keras.preprocessing.sequence.pad_sequences(\n","        [x], maxlen=max_sequence_len-1, padding='pre')\n","    y = tokenizer.texts_to_sequences([target_word])[0]\n","    y = tf.keras.utils.to_categorical(\n","        y, num_classes=len(tokenizer.word_index) + 1)\n","\n","    if len(x) != len(y):\n","        raise ValueError(\n","            \"Mismatch in the number of samples between x and y. Make sure they have the same number of samples.\")\n","\n","    if train_model:\n","        model.fit(x, y, epochs=1, verbose=0)\n","\n"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated Text: Document 1 is about learning learning learning learning processing\n"]}],"source":["\n","# Example usage to generate text\n","generated_text = generate_text(\n","    \"Document 1 is about\", 5, loaded_model, tokenizer, max_sequence_length)\n","print(\"Generated Text:\", generated_text)\n"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["\n","# User provides feedback\n","user_feedback_text = \"Document 1 is about\"\n","user_target_word = \"machine\"\n"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["\n","# Reinforce the model based on user feedback (with training)\n","reinforce_model(user_feedback_text, user_target_word, loaded_model,\n","                tokenizer, max_sequence_length, train_model=True)\n"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated Text after feedback (with training): Document 1 is about learning learning learning processing processing\n","Generated Text after feedback (without training): Document 1 is about learning learning learning processing processing\n"]}],"source":["\n","\n","# Generate text again after reinforcement\n","generated_text_after_feedback = generate_text(\n","    \"Document 1 is about\", 5, loaded_model, tokenizer, max_sequence_length)\n","print(\"Generated Text after feedback (with training):\",\n","      generated_text_after_feedback)\n","\n","# Reinforce the model based on user feedback (without training)\n","reinforce_model(user_feedback_text, user_target_word, loaded_model,\n","                tokenizer, max_sequence_length, train_model=False)\n","\n","# Generate text again after reinforcement (without training)\n","generated_text_after_feedback_no_training = generate_text(\n","    \"Document 1 is about\", 5, loaded_model, tokenizer, max_sequence_length)\n","print(\"Generated Text after feedback (without training):\",\n","      generated_text_after_feedback_no_training)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPhPH9KC52rl2jSW0Xjx3WV","gpuType":"T4","mount_file_id":"12RBqsdsW6ybtYghxXZflWeEbTXLfT92g","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
